{"cells":[{"cell_type":"markdown","source":"# Classifying Violent/Non-Violent Actions\nWe will be using the dataset [`Real Life Violence Situations`](https://paperswithcode.com/dataset/real-life-violence-situations-dataset) found on Papers With Code. The dataset contains 1000 videos of violent actions and 1000 videos of non-violent actions. With the dataset we will train a video classification model to declare if a video is violent or non-violent. \n\nIf the video has a violent action the **goal** is to return the snippet of the video that was declared as violent.","metadata":{"tags":[],"cell_id":"9a368899a4b641e2a67b7fe537eb9f8d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## TODO\n1. Finish the initial model by the end of <mark>Spring Break</mark> (**everyone**)\n    a. Resolve training problem\n2. ~~Finish uploading 394 videos to dataset (**Gueren**)~~","metadata":{"tags":[],"cell_id":"6782c54f0e944e62b05d76297d250327","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Exploring the data\nWe will be using the RVLS dataset. The anomaly detction model will use the avenue dataset.","metadata":{"tags":[],"cell_id":"ca96dcd93e184326a9d7a4b4247a2886","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"!ls /datasets/anomaly-detection/Real\\ Life\\ Violence\\ Dataset -R","metadata":{"tags":[],"cell_id":"ea52a7179b6347acbff7f21b43cf2944","source_hash":"b35db8f6","execution_start":1678949855252,"execution_millis":667,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"ls: cannot access '/datasets/anomaly-detection/Real Life Violence Dataset': No such file or directory\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"As you can see there are 1000 violent and non-violent videos each. We can view one of the videos with IPython's `display` and `HTML` libraries. Below we created a function to display the video so we can look at any videos that could be causing us issues later on.","metadata":{"tags":[],"cell_id":"0deef455884a47e68b066a8e9bfa35af","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"from IPython.display import display, HTML \nfrom base64 import b64encode\n\ndef display_video(path):\n    mp4 = open(path,'rb').read()   \n    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n    display(\n        HTML(\n        \"\"\"\n          <video width=400 controls>\n                <source src=\"%s\" type=\"video/mp4\">\n          </video>\n        \"\"\" % data_url\n        )\n    )","metadata":{"tags":[],"cell_id":"cc23c90858fb41f0942b748a4b440055","source_hash":"3c75a32c","execution_start":1678949855880,"execution_millis":39,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Example of a Non-Violent Action**","metadata":{"tags":[],"cell_id":"651b153bda814322821209859d6e5add","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"display_video('/datasets/anomaly-detection/Real Life Violence Dataset/Testing/NonViolence/NV_999.mp4')","metadata":{"tags":[],"cell_id":"22e4e2ddcf334f31897aa582f2d3dd46","source_hash":"9616cdd7","execution_start":1678949855886,"execution_millis":496,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/datasets/anomaly-detection/Real Life Violence Dataset/Testing/NonViolence/NV_999.mp4'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdisplay_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/datasets/anomaly-detection/Real Life Violence Dataset/Testing/NonViolence/NV_999.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn [2], line 5\u001b[0m, in \u001b[0;36mdisplay_video\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisplay_video\u001b[39m(path):\n\u001b[0;32m----> 5\u001b[0m     mp4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()   \n\u001b[1;32m      6\u001b[0m     data_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:video/mp4;base64,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m b64encode(mp4)\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m      7\u001b[0m     display(\n\u001b[1;32m      8\u001b[0m         HTML(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m     )\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/anomaly-detection/Real Life Violence Dataset/Testing/NonViolence/NV_999.mp4'"]}],"execution_count":3},{"cell_type":"markdown","source":"**Example of a Violent Action**","metadata":{"tags":[],"cell_id":"64fab3f2b90b4ae0acae997c1b4da779","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"display_video('/datasets/anomaly-detection/Real Life Violence Dataset/Training/Violence/V_369.mp4')","metadata":{"tags":[],"cell_id":"f2e2dd94daad46c097bf0870cf36fff1","source_hash":"2c0f8cc8","execution_start":1678861871842,"execution_millis":87984653,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Begin Combining the Hugging Face Tutorial and Surveillance Videos App Here\nFollow along with the tutorial [here.](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb) Just make sure you switch out the model for the one we are using.","metadata":{"tags":[],"cell_id":"fb8b757983534c40a322873a4cfa3d04","source_hash":"5027aa5f","execution_start":1678426746450,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Model\nWe will finetune the [VideoMAE](https://huggingface.co/docs/transformers/main/model_doc/videomae) pretrained on the [Kinetics 400 dataset](https://www.deepmind.com/open-source/kinetics). The benefit of using a pretrained model is that it already has a basic understanding of the inner representations of a video.\n\n### How does VideoMAE work?\nVideoMAE stands for Video Masked Autoencoder, masked autoencoder meaning a scalable self-supervised learner for computer vision. Basically, it removes chunks of an image and (in pre-training) the model must reconstruct raw pixel values. VideoMAE follows the same idea and centers around the following pipeline:\n1. Temporal downsampling\n    - A video clip is randomly sampled from the original video and its frames are compressed/trimmed, each frame containing $$W \\times H \\times 3$$ pixels.\n2. Cube embedding\n    - Each cube is the size $$2 \\times 16 \\times 16$$  **or**  $$time\\ (frames) \\times height \\times width$$. Thus, the cube embedding layer (one frame) produces $${T \\over 2} \\times {H \\over 16} \\times {W \\over 16}$$ 3D tokens, reducing the amount of space and frames from the input.\n3. Tube masking with high ratios\n    - Uses masking ratios between 90% to 95%, meaning that most of the cubes are unused. This helps mitigate information leakage and forces the model to learn more. \n    - Tube masking means that cubes masked in one frame will remain masked in the next frame. This method reduces information leakage compared to the other masking methods. (View image below for example of tube masking)\n4. Backbone\n    - Uses a vison transformer (ViT) as its backbone. \n        - ViT is a model that is used for image classification. It uses self-attention to extract features from an image. Then it splits the images into tokens and transforms them to a linear representation. Then a multi-layer perception (MLP) is used to extract unique features, which helps classify the image.\n    - The ViT is applied to the unmasked cubes and extracts features from them. In pre-training, the model must try to reconstruct the pixels in these frames based on these extracted features. In testing, the extracted features are used form a prediction on which class their belong too.\n\n<div style=\"text-align: center;\"><b>Visual Demonstration</b></div>\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/videomae_architecture.jpeg\"/>\n\n\nAnother model we could use is the [X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip#transformers.XCLIPVisionModel) model.","metadata":{"tags":[],"cell_id":"116aa7aea22f424baa5458dd2e2fdcef","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Epoch \n- Number of complete passes through the dataset\nBatch\n- Number of samples before the model updatees","metadata":{"tags":[],"cell_id":"0963a3966f4c4d899cd0fe46ab35861d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"model_ckpt = \"MCG-NJU/videomae-base\" # pre-trained model from which to fine-tune\nbatch_size = 8 # batch size for training and evaluation","metadata":{"tags":[],"cell_id":"e8087eeeed6248bba3b35e75f5359077","source_hash":"f3c50f3d","execution_start":1678861871901,"execution_millis":87984594,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Loading the dataset\nWe initialize the root path for the dataset with `pathlib`","metadata":{"tags":[],"cell_id":"5302065b165243c0a9a2ab6ba111e71a","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import pathlib\n\ndataset_root_path = \"/datasets/anomaly-detection/Real Life Violence Dataset\"\ndataset_root_path = pathlib.Path(dataset_root_path)","metadata":{"tags":[],"cell_id":"cd0957f9ef8b4b198debc0c4d2614985","source_hash":"c08f2e9b","execution_start":1678861871902,"execution_millis":87984593,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Next we set up `all_video_file_paths` to contain the location of all the videos.","metadata":{"tags":[],"cell_id":"bef31f1774804792bcd2dcedaac9136f","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"all_video_file_paths = (\n    list(dataset_root_path.glob(\"Training/*/*.mp4\")) +\n    list(dataset_root_path.glob(\"Validation/*/*.mp4\")) +\n    list(dataset_root_path.glob(\"Testing/*/*.mp4\"))\n)\n\nall_video_file_paths[:5]","metadata":{"tags":[],"cell_id":"0c81de5d3e8648a0a7375d6b26214438","source_hash":"746c6e93","execution_start":1678861871902,"execution_millis":87984594,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Then, we establish two dictionaries that will be helpful initializing the model.\n\n- `label2id`: maps the class names to integers.\n- `id2label`: maps the integers to class names. ","metadata":{"tags":[],"cell_id":"ca95c3c9bbc947ffb465c717193903bd","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"class_label = sorted({str(path).split(\"/\")[5] for path in all_video_file_paths})\nlabel2id = {label: i for i, label in enumerate(class_label)}\nid2label = {i: label for label, i in label2id.items()}\n\nprint(f\"Unique classes: {list(label2id.keys())}\")","metadata":{"tags":[],"cell_id":"58580d0876c54a05b9ba30fd709e82cf","source_hash":"7580fb31","execution_start":1678861872173,"execution_millis":87984323,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"As we can see we have 2 unique classes, each class having 800 training videos.","metadata":{"tags":[],"cell_id":"79cddba04fb14cb08637f49320563f48","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Loading the model\nIn the next cell, we initialize the `VideoMAE` model where the encoder is initialized with the pre-trained paraemters and the classification head is random initialized. We also initialized the feature extractor for the VideoMAE model, which will be used later.","metadata":{"tags":[],"cell_id":"b159866055b542709cba4ece522d2908","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\n\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(model_ckpt)\nmodel = VideoMAEForVideoClassification.from_pretrained(\n    model_ckpt,\n    label2id = label2id,\n    id2label = id2label,\n    ignore_mismatched_sizes = True, # this is neccessary for fine-tuning models\n)","metadata":{"tags":[],"cell_id":"f450fe376a894231a66dacef36da1058","source_hash":"b31c2ec1","execution_start":1678861872178,"execution_millis":87984318,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"The warnings are telling us that we are throwing away the weights and bias of the `classifier` layer, which are used to classify videos in the pre-trained model. Since we are using different classifications, we must discard those waits and train new waits.","metadata":{"tags":[],"cell_id":"dfea61866364439e9a74fec53a6128f9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Constructing the datasets for training\nFor preprocessing we'll leverage the [PyTorch Video library](https://pytorchvideo.org/). The block below initializes the dependencies we need. ","metadata":{"tags":[],"cell_id":"4758e66fa873463bb6b05883f8c5327a","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import pytorchvideo.data\n\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    Normalize,\n    RandomShortSideScale,\n    RemoveKey,\n    ShortSideScale,\n    UniformTemporalSubsample,\n)\n\nfrom torchvision.transforms import (\n    Compose,\n    Lambda,\n    RandomCrop,\n    RandomHorizontalFlip,\n    Resize,\n)","metadata":{"tags":[],"cell_id":"7da135dd67854793a864a748bd90a96b","source_hash":"a9f17f9b","execution_start":1678861881734,"execution_millis":87974763,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"For the training dataset transformations, we use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. \n\nFor the validation and evaluation dataset transformations, we keep the transformation chain the same except for random cropping and horizontal flipping.\n\nTo learn more about the details of these transformations check out the [official documentation of PyTorch Video](https://pytorchvideo.org). The following blocks follows the [official PyTorch Video example.](https://pytorchvideo.org/docs/tutorial_classification#dataset)  ","metadata":{"tags":[],"cell_id":"c418a2fd403d419fa3cac7d55fdf6fe5","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import os\n\nmean = feature_extractor.image_mean\nstd = feature_extractor.image_std\nresize_to = feature_extractor.size['shortest_edge']\n\nnum_frames_to_sample = model.config.num_frames\nsample_rate = 4\nfps = 30\nclip_duration = num_frames_to_sample * sample_rate / fps\n\n\n# Training dataset transformations.\ntrain_transform = Compose(\n    [\n        ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose(\n                [\n                    UniformTemporalSubsample(num_frames_to_sample),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    RandomShortSideScale(min_size=256, max_size=320),\n                    RandomCrop(resize_to),\n                    RandomHorizontalFlip(p=0.5),\n                ]\n            ),\n        ),\n    ]\n)\n\n# Validation and evaluation datasets' transformations.\nval_transform = Compose(\n    [\n        ApplyTransformToKey(\n            key=\"video\",\n            transform=Compose(\n                [\n                    UniformTemporalSubsample(num_frames_to_sample),\n                    Lambda(lambda x: x / 255.0),\n                    Normalize(mean, std),\n                    Resize((resize_to, resize_to)),\n                ]\n            ),\n        ),\n    ]\n)","metadata":{"tags":[],"cell_id":"fbc78db15c7c42e7984d637010b3d56b","source_hash":"fc37c7c4","execution_start":1678861882243,"execution_millis":87974254,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"We have to set up our own dataset decleration with [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset). This applies the transformations above onto the dataesets we provide it.","metadata":{"tags":[],"cell_id":"32737eed372145bebccb2a37df14c6fe","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Training dataset.\ntrain_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path=os.path.join(dataset_root_path, \"Training\"),\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n    decode_audio=False,\n    transform=train_transform,\n)\n\n# Validation and evaluation datasets.\nval_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path=os.path.join(dataset_root_path, \"Validation\"),\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n    decode_audio=False,\n    transform=val_transform,\n)\n\ntest_dataset = pytorchvideo.data.labeled_video_dataset(\n    data_path=os.path.join(dataset_root_path, \"Testing\"),\n    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n    decode_audio=False,\n    transform=val_transform,\n)","metadata":{"tags":[],"cell_id":"d1c1fffb96e845388909e73f7536d2f5","source_hash":"e8e0617c","execution_start":1678861882247,"execution_millis":87974250,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"We can access the `num_videos` argument to know the number of videos we have in the dataset.","metadata":{"tags":[],"cell_id":"3757f4853ea7455588078c98bf70129b","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos","metadata":{"tags":[],"cell_id":"995f843689e74f7abcd4007c6203904a","source_hash":"4c49aa0a","execution_start":1678861882536,"execution_millis":87973961,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"As you can see our current structure for the dataset is:\n> 80% training\n> 10% validation\n> 10% testing","metadata":{"tags":[],"cell_id":"31d31f3c18b74cf782fcd971123a8633","source_hash":"92750819","execution_start":1678570445208,"execution_millis":210,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Now lets take a look at one of the preprocessed videos.","metadata":{"tags":[],"cell_id":"e4588add291c4fb88881045aff0f0028","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"sample_video = next(iter(train_dataset))\nsample_video.keys()","metadata":{"tags":[],"cell_id":"aa4441c8b96e4c3582ddcbb55b944346","source_hash":"26eb1da4","execution_start":1678861882542,"execution_millis":87973955,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Below, you see that the shape of the video sample starts with 3. The shape will always start with 3 because you need to represent the RGB values. This is the same reason why the trasnformations above divided by 255. The shape follows the format:\n> (RGB, frames, height, width)","metadata":{"tags":[],"cell_id":"8f450888856a4a9285310fb500f1a1e6","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def investigate_video(sample_video):\n    for k in sample_video:\n        if k == \"video\":\n            print(k, sample_video[\"video\"].shape)\n        else:\n            print(k, sample_video[k])\n    print(f\"Video label: {id2label[sample_video[k]]}\")\n\ninvestigate_video(sample_video)","metadata":{"tags":[],"cell_id":"4c7c26230a984908bb886e9611a158df","source_hash":"bfc573e0","execution_start":1678861882614,"execution_millis":87973884,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Now we will create a way to visualize the specific frames in `sample data`. The difference between the function below and 'display_video' is that display_video will show the whole video while `create_gif` only shows the data being used.","metadata":{"tags":[],"cell_id":"82823a65007c4f0e8105e472ae16484e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import imageio\nimport numpy as np\nfrom IPython.display import Image\n\ndef unnormalize_img(img):\n    img = (img * std) + mean\n    img = (img * 255).astype(\"uint8\")\n    return img.clip(0, 255)","metadata":{"tags":[],"cell_id":"8784c32aaca64cdbb13acd367f2b897d","source_hash":"ff73bb12","execution_start":1678861882667,"execution_millis":87973834,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"The function `unnormalize_img` reverses the transformations by\n```\nCompose[\n    Lambda(lambda x: x / 255.0),\n    Normalize(mean, std)\n]","metadata":{"tags":[],"cell_id":"9db09a8e405244a396ff5eaa292d24c6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Prepares a GIF from a video tensor. The video tensor is expected to have the following shape:\n> (num_frames, num_channels, height, width).","metadata":{"tags":[],"cell_id":"2c58e6f8957a4feaa461baca7261f169","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def create_gif(video_tensor, filename=\"sample.gif\"):\n    frames = []\n    for video_frame in video_tensor:\n        frame_unnormalized = unnormalize_img(video_frame.permute(1,2,0).numpy())\n        frames.append(frame_unnormalized)\n    kargs = {\"duration\": (len(video_tensor)/60)} # num frames / 60 fps\n    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n    return filename","metadata":{"tags":[],"cell_id":"8e93c20d3afe4dd58bcabd4c7767a3d7","source_hash":"4636f5b3","execution_start":1678861882667,"execution_millis":87973871,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Prepares and displays a GIF from a video tensor.","metadata":{"tags":[],"cell_id":"568c6ef6c269470a89e4d6a4949abbbb","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def display_gif(video_tensor, gif_name=\"sample.gif\"):\n    video_tensor = video_tensor.permute(1, 0, 2, 3)\n    gif_filename = create_gif(video_tensor, gif_name)\n    return Image(filename=gif_filename)","metadata":{"tags":[],"cell_id":"3bf601622fc14f92bf86adc92fb4d6db","source_hash":"192340f5","execution_start":1678861882668,"execution_millis":87973902,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"code","source":"video_tensor = sample_video[\"video\"]\ndisplay_gif(video_tensor)","metadata":{"tags":[],"cell_id":"a586de176534435e9c6820c3ab887aa2","source_hash":"9155aa72","execution_start":1678861882669,"execution_millis":87973902,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Training the Model","metadata":{"tags":[],"cell_id":"88dc7d427ec44463a093016ee24cfd2f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Before training the model, we need to prepare and set up arguments that will be passed to the model","metadata":{"tags":[],"cell_id":"a36b9b6698b649b58029900c14e63924","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"!pip show transformers\nfrom transformers import TrainingArguments, Trainer\n\n# set up model name\nmodel_name = model_ckpt.split(\"/\")[-1]\nnew_model_name = f\"{model_name}-finetuned-RealLifeViolenceSituations-subset\"\nnum_epochs = 4\n\n# set up a subset of arguments\nargs = TrainingArguments(\n    new_model_name,\n    remove_unused_columns=False,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    warmup_ratio=0.1,\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    push_to_hub=True,\n    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n)","metadata":{"tags":[],"cell_id":"0ab391cbe0bf48bfa4e0622853c17eba","source_hash":"940b0332","execution_start":1678861883424,"execution_millis":87973147,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Load a type of metric that we want to measure our model: accuracy","metadata":{"tags":[],"cell_id":"f38a3b2d56244d6e88f68c4c62dfc0e5","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"accuracy\")","metadata":{"tags":[],"cell_id":"65340313d6aa49a0956496028a24eb5c","source_hash":"ec17e047","execution_start":1678861884208,"execution_millis":87972363,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Define a function to compute accuracy. This function will later be passed to the Trainer as an argument. ","metadata":{"tags":[],"cell_id":"ec8db984642f49dfadcea81ca01507f8","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# the compute_metrics function takes a Named Tuple as input:\n# predictions, which are the logits of the model as Numpy arrays,\n# and label_ids, which are the ground-truth labels as Numpy arrays.\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions.\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"tags":[],"cell_id":"c394a1fd61c34b4a80e53f1d52e0555f","source_hash":"70a66e7e","execution_start":1678861884379,"execution_millis":87972192,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Define a function that batches examples together. It will be passed into Trainer as an argument.\nEach batch has two keys: pixel_values and labels","metadata":{"tags":[],"cell_id":"41eaf42ee03d46e2916b84965636f19b","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import torch\n\n\ndef collate_fn(examples):\n    \"\"\"The collation function to be used by `Trainer` to prepare data batches.\"\"\"\n    # permute to (num_frames, num_channels, height, width)\n    pixel_values = torch.stack(\n        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n    )\n    labels = torch.tensor([example[\"label\"] for example in examples])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"tags":[],"cell_id":"70832c5cb11e46878d80f49f555dd6c3","source_hash":"2c453aec","execution_start":1678861884385,"execution_millis":87972187,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Create Trainer object. This will be our model.\nThe below block needs token from Hugging Face to execute.\nToken is used: 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\n\nThere are some packages required to be installed in order to loggin to HuggingFace and provide token.","metadata":{"tags":[],"cell_id":"4414c24366ab4a078197318465686d33","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"\n!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n!apt-get install git-lfs\n!git-lfs install\n!pip install huggingface_hub\n!pip install ipywidgets\n# loggin to HuggingFace with token\n!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\"\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n    data_collator=collate_fn,\n)","metadata":{"tags":[],"cell_id":"244762d7c1024165b3757b20fd2481d2","source_hash":"b07627d5","execution_start":1678861884391,"execution_millis":87972181,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Train the model\n\n***Note: I got this error while training:\n\n_A kernel interruption error usually occurs for one of the following reasons:\nThe kernel process runs out of RAM. In this case, click here to upgrade your machine.\nThere is a bug in one of the libraries (e.g., version conflicts, missing binary dependency, etc_\n\nI think it's more likely that we runs out of RAM because I notice our RAM is all the way up to 5 GB and then this error pops up. Our basic plan has only 5 GB.","metadata":{"tags":[],"cell_id":"5042799057be4f028113f393a6a85744","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"train_results = trainer.train()","metadata":{"tags":[],"cell_id":"2e12a00eedef4e95b6dd8bdf5744784b","source_hash":"a4674717","execution_start":1678861898979,"execution_millis":87957593,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"***Code from here and below, cannot be executed because of the code above has problem executing***\n\nEvaluate the model with the test dataset\n","metadata":{"tags":[],"cell_id":"08743ff35004410b9c78184fc572777c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"trainer.evaluate(test_dataset)","metadata":{"tags":[],"cell_id":"bd3c12fdc7e4404fb5888ca10b434e2a","source_hash":"c9908437","execution_start":1678860557032,"execution_millis":89299540,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Save model from the trainer\nSave the evaluation of testing dataset on the model\n","metadata":{"tags":[],"cell_id":"ab3efd269b874feb99137c2d13bee556","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"trainer.save_model()\ntest_results = trainer.evaluate(test_dataset)\ntrainer.log_metrics(\"test\", test_results)\ntrainer.save_metrics(\"test\", test_results)\ntrainer.save_state()\n\n# upload to hub\ntrainer.push_to_hub()","metadata":{"tags":[],"cell_id":"f460b4a8d5a24af1b6fe90eae1a806b9","source_hash":"2d974c37","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Interface","metadata":{"tags":[],"cell_id":"e563dddcbc9741688a8fa646a5026764","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"After saving our trained model, we load it again to classify our dataset","metadata":{"tags":[],"cell_id":"432643012d4e412a85d5dcc2655621ef","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"trained_model = VideoMAEForVideoClassification.from_pretrained(new_model_name)","metadata":{"tags":[],"cell_id":"ba8f42a20e6e4a49bd85e38d95615290","source_hash":"1a8d4ce6","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Display all features of the first video in the dataset","metadata":{"tags":[],"cell_id":"3331476da5f04188aa9b45e792e3bd0c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"sample_test_video = next(iter(test_dataset))\ninvestigate_video(sample_test_video)","metadata":{"tags":[],"cell_id":"c1ad8e44b0094642812d1068536a89f0","source_hash":"65e0d16f","execution_start":1678861858355,"execution_millis":87998219,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Define a function that accepts a model and a video, where the model will classifies the video.\n\nThe logit values for both violence and non-violence will be returned. Later, we will only use the highest one.","metadata":{"tags":[],"cell_id":"c00f5d64c582418884bbad02283dfb67","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def run_inference(model, video):\n    \"\"\"Utility to run inference given a model and test video.\n    \n    The video is assumed to be preprocessed already.\n    \"\"\"\n    # (num_frames, num_channels, height, width)\n    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n\n    inputs = {\n        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n        \"labels\": torch.tensor(\n            [sample_test_video[\"label\"]]\n        ),  # this can be skipped if you don't have labels available.\n    }\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    model = model.to(device)\n\n    # forward pass\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    return logits","metadata":{"tags":[],"cell_id":"52aa849c032a45688d5e531bbb932c35","source_hash":"d6466d2c","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Call the function above, pass the trained model and the sample video obtained before defining the funciton.","metadata":{"tags":[],"cell_id":"50aac7c5e757434b8f8ffd8f3e85e45e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"logits = run_inference(trained_model, sample_test_video[\"video\"])","metadata":{"tags":[],"cell_id":"7ded9bb06cd84978a6fad5183ca3fed1","source_hash":"cde9d4a6","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"Displaying gif of the sample test video","metadata":{"tags":[],"cell_id":"8e10ef0dabc543d4a563b3394bfdf362","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"display_gif(sample_test_video[\"video\"])","metadata":{"tags":[],"cell_id":"3fcc8bbcea904437b6015a038abbf8f8","source_hash":"872474f9","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"The maximum logit values, returned by the run_inference function, will be used. We won't use the logit value but its index along the axis -1 (the last dimension). This index will be converted into a label and displayed.","metadata":{"tags":[],"cell_id":"b42217069423495cac61b56bce56e0ae","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"predicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])","metadata":{"tags":[],"cell_id":"a9cd80dd5221410e81d511c8225a33e6","source_hash":"b45c4bec","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6c0d6237-fce8-4a74-a541-a7cac87607e4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"e338c8f3236345dba2de322e36b2062a","deepnote_execution_queue":[]}}